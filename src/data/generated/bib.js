define({ entries : {
    "AlhijawiBushra2024DLDM": {
        "abstract": "Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual content is written and communicated. These models have the potential to generate scientific content that is indistinguishable from that written by humans. Hence, LLMs carry severe consequences for the scientific community, which relies on the integrity and reliability of publications. This research paper presents a novel ChatGPT-generated scientific text detection method, AI-Catcher. AI-Catcher integrates two deep learning models, multilayer perceptron (MLP) and convolutional neural networks (CNN). The MLP learns the feature representations of the linguistic and statistical features. The CNN extracts high-level representations of the sequential patterns from the textual content. AI-Catcher is a multimodal model that fuses hidden patterns derived from MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset is collected to enhance AI-generated text detection tools, AIGTxt. AIGTxt contains 3000 records collected from published academic articles across ten domains and divided into three classes: Human-written, ChatGPT-generated, and Mixed text. Several experiments are conducted to evaluate the performance of AI-Catcher. The comparative results demonstrate the capability of AI-Catcher to distinguish between human-written and ChatGPT-generated scientific text more accurately than alternative methods. On average, AI-Catcher improved accuracy by 37.4%.",
        "address": "Ithaca",
        "author": "Alhijawi, Bushra and Jarrar, Rawan and AbuAlRub, Aseel and Bader, Arwa",
        "copyright": "2024. This work is published under http://creativecommons.org/licenses/by-nc-nd/4.0/ (the \u201cLicense\u201d). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.",
        "issn": "2331-8422",
        "journal": "arXiv.org",
        "keywords": "Artificial intelligence ; Artificial neural networks ; Chatbots ; Deep learning ; Large language models ; Machine learning ; Multilayer perceptrons ; Representations",
        "language": "eng",
        "publisher": "Cornell University Library, arXiv.org",
        "title": "Deep Learning Detection Method for Large Language Models-Generated Scientific Content",
        "type": "article",
        "year": "2024"
    },
    "ElkhatatAhmedM": {
        "abstract": "The proliferation of artificial intelligence (AI)-generated content, particularly from models like ChatGPT, presents potential challenges to academic integrity and raises concerns about plagiarism. This study investigates the capabilities of various AI content detection tools in discerning human and AI-authored content. Fifteen paragraphs each from ChatGPT Models 3.5 and 4 on the topic of cooling towers in the engineering process and five human-witten control responses were generated for evaluation. AI content detection tools developed by OpenAI, Writer, Copyleaks, GPTZero, and CrossPlag were used to evaluate these paragraphs. Findings reveal that the AI detection tools were more accurate in identifying content generated by GPT 3.5 than GPT 4. However, when applied to human-written control responses, the tools exhibited inconsistencies, producing false positives and uncertain classifications. This study underscores the need for further development and refinement of AI content detection tools as AI-generated content becomes more sophisticated and harder to distinguish from human-written text.",
        "address": "Singapore",
        "author": "Elkhatat, Ahmed M. and Elsaid, Khaled and Almeer, Saeed",
        "copyright": "The Author(s) 2023",
        "issn": "1833-2595",
        "journal": "International Journal for Educational Integrity",
        "keywords": "Academic integrity ; AI content detection tools ; AI-generated content ; Artificial Intelligence ; Chatbots ; ChatGPT ; Education ; Education & Educational Research ; Educational Technology ; Ethics ; Higher Education ; Identification ; Integrity ; International and Comparative Education ; Original Article ; Plagiarism ; Social Sciences ; Writing (Composition)",
        "language": "eng",
        "number": "1",
        "pages": "17--16",
        "publisher": "Springer Nature Singapore",
        "title": "Evaluating the efficacy of AI content detection tools in differentiating between human and AI-generated text",
        "type": "article",
        "volume": "19",
        "year": "2023"
    },
    "HuXiaomeng2023RRAD": {
        "abstract": "Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo.",
        "address": "Ithaca",
        "author": "Hu, Xiaomeng and Pin-Yu, Chen and Tsung-Yi, Ho",
        "copyright": "2023. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the \u201cLicense\u201d). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.",
        "issn": "2331-8422",
        "journal": "arXiv.org",
        "keywords": "Artificial intelligence ; Large language models ; Radar detection ; Robustness ; Sensors ; Texts",
        "language": "eng",
        "publisher": "Cornell University Library, arXiv.org",
        "title": "RADAR: Robust AI-Text Detection via Adversarial Learning",
        "type": "article",
        "year": "2023"
    },
    "JawaharGanesh2020ADoM": {
        "abstract": "Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.",
        "address": "Ithaca",
        "author": "Jawahar, Ganesh and Abdul-Mageed, Muhammad and Lakshmanan, Laks V S",
        "copyright": "2020. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the \u201cLicense\u201d). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.",
        "issn": "2331-8422",
        "journal": "arXiv.org",
        "keywords": "Detectors ; Error analysis ; Literature reviews ; Machine learning ; Natural language processing ; Product reviews",
        "language": "eng",
        "publisher": "Cornell University Library, arXiv.org",
        "title": "Automatic Detection of Machine Generated Text: A Critical Survey",
        "type": "article",
        "year": "2020"
    },
    "KrishnaKalpesh2023Pedo": {
        "abstract": "The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Using DIPPER to paraphrase text generated by three large language models (including GPT3.5-davinci-003) successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics. To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data.",
        "address": "Ithaca",
        "author": "Krishna, Kalpesh and Song, Yixiao and Karpinska, Marzena and Wieting, John and Iyyer, Mohit",
        "copyright": "2023. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the \u201cLicense\u201d). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.",
        "issn": "2331-8422",
        "journal": "arXiv.org",
        "keywords": "Algorithms ; Detectors ; Dipping ; Knobs ; Semantics ; Sensors ; Source code",
        "language": "eng",
        "publisher": "Cornell University Library, arXiv.org",
        "title": "Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense",
        "type": "article",
        "year": "2023"
    },
    "MaoChengzhi2024RgAD": {
        "abstract": "We find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output. We dubbed our geneRative AI Detection viA Rewriting method Raidar. Raidar significantly improves the F1 detection scores of existing AI content detection models -- both academic and commercial -- across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points. Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content. Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves.",
        "address": "Ithaca",
        "author": "Mao, Chengzhi and Vondrick, Carl and Wang, Hao and Yang, Junfeng",
        "copyright": "2024. This work is published under http://creativecommons.org/licenses/by-nc-nd/4.0/ (the \u201cLicense\u201d). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.",
        "issn": "2331-8422",
        "journal": "arXiv.org",
        "keywords": "Generative artificial intelligence ; Large language models",
        "language": "eng",
        "publisher": "Cornell University Library, arXiv.org",
        "title": "Raidar: geneRative AI Detection viA Rewriting",
        "type": "article",
        "year": "2024"
    },
    "ProvaNuzhat2024DAGT": {
        "abstract": "Recent advances in natural language processing (NLP) may enable artificial intelligence (AI) models to generate writing that is identical to human written form in the future. This might have profound ethical, legal, and social repercussions. This study aims to address this problem by offering an accurate AI detector model that can differentiate between electronically produced text and human-written text. Our approach includes machine learning methods such as XGB Classifier, SVM, BERT architecture deep learning models. Furthermore, our results show that the BERT performs better than previous models in identifying information generated by AI from information provided by humans. Provide a comprehensive analysis of the current state of AI-generated text identification in our assessment of pertinent studies. Our testing yielded positive findings, showing that our strategy is successful, with the BERT emerging as the most probable answer. We analyze the research's societal implications, highlighting the possible advantages for various industries while addressing sustainability issues pertaining to morality and the environment. The XGB classifier and SVM give 0.84 and 0.81 accuracy in this article, respectively. The greatest accuracy in this research is provided by the BERT model, which provides 0.93% accuracy.",
        "address": "Ithaca",
        "author": "Prova, Nuzhat",
        "copyright": "2024. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the \u201cLicense\u201d). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.",
        "issn": "2331-8422",
        "journal": "arXiv.org",
        "keywords": "Accuracy ; Artificial intelligence ; Classifiers ; Deep learning ; Machine learning ; Natural language processing ; Support vector machines",
        "language": "eng",
        "publisher": "Cornell University Library, arXiv.org",
        "title": "Detecting AI Generated Text Based on NLP and Machine Learning Approaches",
        "type": "article",
        "year": "2024"
    },
    "SadasivanVinuSankar2024CATb": {
        "abstract": "The unregulated use of LLMs can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, we show that these detectors are not reliable in practical scenarios. In particular, we develop a recursive paraphrasing attack to apply on AI text, which can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors, zero-shot classifiers, and retrieval-based detectors. Our experiments include passages around 300 tokens in length, showing the sensitivity of the detectors even in the case of relatively long passages. We also observe that our recursive paraphrasing only degrades text quality slightly, measured via human studies, and metrics such as perplexity scores and accuracy on text benchmarks. Additionally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks aimed to mislead detectors to classify human-written text as AI-generated, potentially causing reputational damages to the developers. In particular, we show that an adversary can infer hidden AI text signatures of the LLM outputs without having white-box access to the detection method. Finally, we provide a theoretical connection between the AUROC of the best possible detector and the Total Variation distance between human and AI text distributions that can be used to study the fundamental hardness of the reliable detection problem for advanced language models. Our code is publicly available at https://github.com/vinusankars/Reliability-of-AI-text-detectors.",
        "address": "Ithaca",
        "author": "Sadasivan, Vinu Sankar and Kumar, Aounon and Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil",
        "copyright": "2024. This work is published under http://creativecommons.org/licenses/by/4.0/ (the \u201cLicense\u201d). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.",
        "issn": "2331-8422",
        "journal": "arXiv.org",
        "keywords": "Classifiers ; Detectors ; Human performance ; Large language models ; Neural networks ; Polynomials ; Pseudorandom ; Sensors ; Spoofing ; Watermarking",
        "language": "eng",
        "publisher": "Cornell University Library, arXiv.org",
        "title": "Can AI-Generated Text be Reliably Detected?",
        "type": "article",
        "year": "2024"
    },
    "SharifOmar2020Dstu": {
        "abstract": "Due to the substantial growth of internet users and its spontaneous access via electronic devices, the amount of electronic contents has been growing enormously in recent years through instant messaging, social networking posts, blogs, online portals and other digital platforms. Unfortunately, the misapplication of technologies has increased with this rapid growth of online content, which leads to the rise in suspicious activities. People misuse the web media to disseminate malicious activity, perform the illegal movement, abuse other people, and publicize suspicious contents on the web. The suspicious contents usually available in the form of text, audio, or video, whereas text contents have been used in most of the cases to perform suspicious activities. Thus, one of the most challenging issues for NLP researchers is to develop a system that can identify suspicious text efficiently from the specific contents. In this paper, a Machine Learning (ML)-based classification model is proposed (hereafter called STD) to classify Bengali text into non-suspicious and suspicious categories based on its original contents. A set of ML classifiers with various features has been used on our developed corpus, consisting of 7000 Bengali text documents where 5600 documents used for training and 1400 documents used for testing. The performance of the proposed system is compared with the human baseline and existing ML techniques. The SGD classifier \u2018tf-idf\u2019 with the combination of unigram and bigram features are used to achieve the highest accuracy of 84.57%.",
        "address": "Basel",
        "author": "Sharif, Omar and Hoque, Mohammed Moshiul and Kayes, A. S.M. and Nowrozy, Raza and Sarker, Iqbal H.",
        "copyright": "Copyright 2023 Elsevier B.V., All rights reserved.",
        "issn": "2076-3417",
        "journal": "Applied sciences",
        "keywords": "Abuse ; Accuracy ; Algorithms ; Bengali language processing ; Classifiers ; Cyberbullying ; Datasets ; Deep learning ; Electronic equipment ; Extremism ; Feature extraction ; Hate speech ; Instant messaging systems ; Internet ; Language ; Learning algorithms ; Machine learning ; National security ; Natural language processing ; Neural networks ; Social networks ; Social organization ; Suspicious corpora ; Suspicious text detection ; Text classification ; Threats",
        "language": "eng",
        "number": "18",
        "pages": "6527",
        "publisher": "MDPI AG",
        "title": "Detecting suspicious texts using machine learning techniques",
        "type": "article",
        "volume": "10",
        "year": "2020"
    },
    "WaltersWilliamH": {
        "abstract": "This study evaluates the accuracy of 16 publicly available AI text detectors in discriminating between AI-generated and human-generated writing. The evaluated documents include 42 undergraduate essays generated by ChatGPT-3.5, 42 generated by ChatGPT-4, and 42 written by students in a first-year composition course without the use of AI. Each detector\u2019s performance was assessed with regard to its overall accuracy, its accuracy with each type of document, its decisiveness (the relative number of responses), the number of false positives (human-generated papers designated as by the detector), and the number of false negatives (AI-generated papers designated as ). Three detectors \u2013 Copyleaks, TurnItIn, and Originality.ai \u2013 have high accuracy with all three sets of documents. Although most of the other 13 detectors can distinguish between GPT-3.5 papers and human-generated papers with reasonably high accuracy, they are generally ineffective at distinguishing between GPT-4 papers and those written by undergraduate students. Overall, the detectors that require registration and payment are only slightly more accurate than the others.",
        "author": "Walters, William H.",
        "copyright": "Copyright 2023 Elsevier B.V., All rights reserved.",
        "issn": "2451-1781",
        "journal": "OPEN INFORMATION SCIENCE",
        "keywords": "AI content detector ; AI writing detector ; artificial intelligence ; chatbot ; generative AI",
        "language": "eng",
        "number": "1",
        "pages": "article e001568--268",
        "publisher": "De Gruyter",
        "title": "The Effectiveness of Software Designed to Detect AI-Generated Writing: A Comparison of 16 AI Text Detectors",
        "type": "article",
        "volume": "7",
        "year": "2023"
    }
}});